A direct consequence of the abundance of works in face elicitation in humans is the amount of works done in Human-Robot Interaction (HRI) focused also on faces. One of the most well-known expressive robots is Kismet~\cite{Breazeal2002}, a robotic face able to interact with people and to show emotions. The face had enough degrees of freedom to portray the basic emotions suggested by Ekman~\cite{Ekman2004} (\textit{happiness}, \textit{surprise}, \textit{anger}, \textit{disgust}, \textit{fear}, and \textit{sadness}) plus \textit{interest}.  Despite the complex system behind Kismet, the emotion's projection evaluation was done using videos with a very limited number of participants. In this experiment, each participant was asked to look at seven videos, one for each implemented emotion. After a video was projected, each participant selected a emotion from a list, which contained seven implemented emotions. The results showed high percentage of recognition (over 57\%).

%%%%%%%%%%%%%%%%%%%%%%%%
Saerbeck and Christoph~\cite{Saerbeck2010} analyzed the relationship between robots' motion characteristics and perceived affect. They first did a literature review to select characteristics that could be used to show affection. From their review, they decided to use acceleration and curvature on robot's trajectory. 
Using these two characteristics, they did an experiment with eighteen participants. They used two different platforms (iCat and Roomba) to verify if the embodiment had any impact on affection determination. Each participant was exposed to both embodiments and all possible variable combinations. To assess participants' perception, they used PANAS~\cite{WatsonClarkTellegen88} and Self-Assessment Manikin (SAM)~\cite{Lang2008}. Their results show that there was no significant difference between the two embodiments used and that participants were able to assign different emotions to the movement patterns shown to them. 
More importantly, they found out that acceleration is correlated with the perceived arousal but not with valence. 

%%%%%%%%%%%%%%%%
Ca\~namero and collaborators~\cite{Canamero2010,Beck2010} studied the perception of key body poses designed to show emotions using a NAO~\cite{NAO2013}. They suggested that techniques used to convey emotions in virtual characters could not be used in robots, due to the fact that virtual characters have no physical constraints, while robots are constrained by their physical capabilities~\cite{Saerbeck2007, Canamero2010}. They proposed a set of poses that could be used to express emotions with a NAO, paying a particular attention to head's contribution. Their results showed a recognition rate of 88\%, 85\%, 92\%, 88\%, 73\%, and 73\% for anger, sadness, fear, pride, happiness and excitement respectively. The major finding from their experiments is that moving the head up improved the identification of pride, happiness, and excitement. While moving the head down improved the identification of anger and sadness. 

%%%%%%%%%%%%%%%%%%%%%%
Daryl~\cite{Arras2012} is an anthropomorphic robot, without facial expression, nor limbs, but with mobility capabilities. This platform was used to test whether it is possible to project emotions without using cues used in a human-like platform (e.g., tail, ears and head). This robot has head, ears, ability to generate colors in a RGB-LED set positioned in its chest, and a speaker system. The emotions implemented were: happiness, sadness, fear, curiosity, disappointment, and embarrassment. During the experiments done by the researchers, special attention was put to the final distance between the robot and the subject. The subjects were exposed to a sequence of movements. For each sequence the subject was asked to rate the intensity of each emotion enlisted in a five-point Likert scale questionnaire. The enlisted emotions were the six implemented plus anger, disgust and surprise. Their results showed that participants gave a higher intensity to the desired emotion, also when similar ones emotions were presented, such as sadness and disappointment. 

%%%%%%%%%%%%%%%%%%%%%%%
Using the platform WABIAN-2R, Destephe and collaborators~\cite{Destephe2013b} studied the attribution of emotion to a robot's gait. To obtain  robot's movements, the researchers asked two professional actors to walk in a room conveying anger, happiness, sadness, and fear with different intensities (low, regular, normal, high, and exaggerated). All the actors' walking were recorded using a Cortex motion capture system. These data were later reported to robot's embodiment. Each subject was exposed to a series of videos. After each video, the subject was asked to select one emotion from a list and state the intensity of the selected emotion. The videos showed were not made with the real robot, but with a virtual model of the robot. Their results showed that sadness was recognized 73.81\% (average) of the times with an intensity of 21.43\%, happiness was recognized 66.67\% (average) of the times with an intensity of 30.95\%, anger was recognized 61.9\% of the times with an intensity of 26.19\%, and fear was recognized 83.33\% (average) of the times with an intensity of 28.58\%. These results show that people could reasonably well recognize emotions from the robot's gait.

%%%%%%%%%%%%%%%%% %
Brown and Howard focused their interest on head and arm movements using a DARwIn-OP platform~\cite{Brown2014}. Their hypothesis was that using some principles, that they determined as important, it is possible to express happiness and sadness in a way that people could recognize. These principles establish that to show happiness is necessary to move robot's head up, arms up, and it should be done fast. While to express sadness the head goes down, arms down, and it should be done slow. To test their hypothesis they conducted an experiment with thirteen participants. Each participant was exposed to fifteen sequences of poses and he/she had to answer for each sequence a five-point Likert scale (very-sad to very happy, with neutral in the middle). They obtained a 95\%, 59\%, and 94\% of accuracy for happiness, neutral and sadness respectively. 
 
%%%%%%%%%%%%%%%%%%%
Sharma and collaborators~\cite{Sharma2013} used a quadrotor to study how different Laban's effort~\cite{Laban1968} parameters could impact on the perception of affection. A professional Laban certified actor was asked to generate 16 different paths, for each one changing one of the four Laban's parameters (space, weight, time, and flow). Each generated path was recorded using the Vicon motion-tracking system. Then, they did an experiment were people were asked to assess each path using the Self-assessment Manikin (SAM)~\cite{Lang2008}. In order to analyse the results, they mapped them on the circumplex model of emotion, and evaluated the contribution of each Laban's parameter in the 2-D circumplex model (arousal and valence). The results show that it is possible to increase both valence and arousal by using a more indirect space, or by performing the motion more quickly, and to decrease valence or arousal by a more direct use of space, or a more sustained motion. Although they suggest the use of Laban's description as a tool to specify affection movements in Human-Robot Interaction, how to use these parameters in the actual implementation remains an open question, since Laban defined them qualitatively, with reference to human people, and they leave very open questions about the most appropriate numerical values.

%%%%%%%%%%%%%%%%%%%
Suk and e.t. studied the human emotional interpretation for speed, smoothness, granularity and volume of a non-human or animal like object~\cite{NAM2014}. Two experiments were designed to determine the relationship among these features and the emotional interpretation. To assess participants' emotional response they used SAM. The first experiment was focused on speed and smoothness movement features, selecting five different values for each one of these two features. Each participant was exposed to twenty five movements. After a participant observed a movement, the participant marked the two SAM graphic figures (pleasure and arousal).
The results from this first experiment show that the arousal increases as speed increases, but that there is not any clear tendency for smoothness. In the second experiment they evaluated the other two features (granularity and volume) using the same procedure followed in the first experiment. Their results show that granularity is positively correlated with pleasure and arousal. On the other volume is negatively correlated with pleasure and positively correlated with arousal. As overall result, they found a major contribution of speed on arousal and minor contribution of granularity and volume.

%%%%%%%%%%%%%%%%%%%
To improve the coordination between humans and robots Novikova and Watss~\cite{Novika2015} proposed the use of emotion. They did studies using a own built platform called E4 to verify whether people could detect emotions from a non-human like platform. The E4 platform was constructed using a Lego Mindstorms NTX and it was based on a Phobot robot's design. In all their experiments they used six emotional expressions (scared, surprised, excited, angry, happy, sad) and neutral. The emotion list used in their experiments was balanced using the 2-D circumplex model (arousal and valence), selecting three from each quadrant for a total of twelve emotions, plus the options of ``other'' and ``don't know''. They obtained a recognition rate of 52\%, 42\%, 41\%, 36\% and 15\% for surprise, fear, sadness, happiness, and anger respectively.