Emotions and mental states are not just expressed through facial expression but also by body through postures and other features~\cite{Gelder2008}. However  many psychological studies have been mostly focused on understanding the role of human face in emotion projection~\cite{Ekman2004},~\cite{kleinsmith2012affective} and mental states. This trend has been followed by the robotics community, where anthropomorphic faces (e.g.,~\cite{Arras2012},~\cite{Breazeal2002}) and bodies (e.g.,~\cite{Canamero2010},~\cite{haering2011},~\cite{Destephe2013}) have been used to convey emotions.
Nevertheless in many situations the presence of anthropomorphic elements would be out of place and not justified by the main robot's functionalities. Most of the current and future robotics platforms on the market will not require anthropomorphic faces~\cite{Breazeal2002} or limbs~\cite{Li2011} , in some cases like, for instance, in floor cleaning robots, anthropomorphic characteristics could even be detrimental to robot's task accomplishment.
This generates the necessity to study other mechanisms that could help to project emotions, which could give people an idea about the robots' state, and engage the user in long term relations.

The amount of works studying non-anthropomorphic features in robotics (e.g.,~\cite{Saerbeck2010,Lakatos2014,Sharma2013,Novika2015}) remains still small in comparison to those that exploit anthropomorphic features. Moreover, these works do not give specific range of values for the characteristics used to expressed the emotions implemented by them. For example Suk and collaborators~\cite{NAM2014} in their study gives specific values for acceleration, curvature, and but their connection to specific emotions is not given. Rather, they gave a relationship between their features and values in terms of pleasure/arousal dimensions. This could lead to select features to convey certain emotions but it would be even better to know specific range of values for those features to convey certain emotions or to know the values that could be misinterpreted with negative emotions.
 
In order to get a better understanding of other features and values that could be used to convey specific emotions, this paper presents an experiment that was designed to identify specific values for some movement features that could be used to express the following emotions, selected among the ones suggested by Ekman as basic~\cite{Ekman2004}: happiness, anger, fear and sadness. The considered features are oscillation angle, linear and angular velocity, direction and orientation, identified as independent variables. The perceived emotions and their intensities were considered as dependent variables. It is to be observed that it was given the possibility to the subjects to provide their evaluation of emotional intensity for more than one emotion for each experience. The experiment took place in Politecnico di Milano during June and July of 2015, where students from diverse departments were asked to participate without any economical retribution. Krippendorff's alpha agreement~\cite{Krippendorff2007} ($\alpha$) was used to evaluate the agreement among the participants for each treatment. For each of the four emotions was generated a top 10 list based on alpha agreement and perceived intensity. The results suggest that fear is perceived when the robot is looking at them and moving far from them fast. Sadness is attributed to slow velocities with slow angular velocity and small oscillation angle. Anger is attributed to fast velocities, both angular and linear, small angle of oscillation and the robot facing the person when it is approaching them.

This paper is organized as follows. The next section introduces previous studies done in human emotion and robot emotions projection. Section~\ref{sec:system} introduces the robotic platform and the software used in the experiment. The experiment's design is explained in section~\ref{sec:experiment}. Finally sections~\ref{sec:study} and~\ref{sec:result} presents the study done and the obtained results.