
Although it is not possible to apply a direct mapping from human studies to robots~\cite{Saerbeck2007,Canamero2010}, human studies provide guidelines about possible features and values that could be used to generate emotional motion. 
 
\subsection{Human Studies}

Emotion plays an important role in human-human interaction, and can be expressed through diverse channels such as body gestures and poses, body movements, face expressions. Given the complex structure of human face, where more than 43 muscles act, face configurations are considered as a primary channel to express emotion. As a consequence, many works have focused on facial expression, mainly, but not only, influenced by the work done by Ekman~\cite{Ekman2004}. Fortunately, the role that body plays in emotions projection has been recognized and has started to be studied~\cite{Gelder2008,Wallboot1998}. Nevertheless, the amount of works related to body expression of emotions is still small compared to studies on facial expression. 

Analysing some of the few works that have studied human body expressiveness, it is possible to recognize two different methodologies to create the data base of movements to be assessed during the experiments. Applying a first methodology, human actors (either professionals, or amateurs) are asked to walk straight from point A to point B conveying specific emotions~\cite{Dael2012,Meijer1989,Wallboot1998}. Each trial is recorded and later shown to each subject, who evaluates all the sequences. The second methodology uses virtual agents~\cite{Roether2009,Venture2014} to generate the very same set up for the experiments; the agent's movements are generated from the data recorded from human actors.

The work done by Wallboot~\cite{Wallboot1998} has been used as a referenced by others.
His work addressed the question: \textit{Are specific body postures indicative of emotion or are they only used to indicate the intensity of the emotion?} To answer this question, he recorded 224 videos for joy, happiness, sadness, despair, fear, terror, coldness anger, hot anger, disgust, contempt, shame, guilt, pride, and boredom, and showed them to the subjects. His results reaffirm that movement and body postures are indicative of intensity for some emotions, while, for other emotions, these two characteristics seem to be enough to identify them.

Complementary studies using virtual agents have been performed by  Kluwer and collaborators~\cite{Kluwer2004}, who studied the contribution of postures and angle view in the interpretation of emotions. They generated 176 static positions for happiness, anger, disgust, fear, sadness and surprise. Each image was later rendered from three different angles (front, left, and above and behind left shoulder) producing a total of 528 images. All the participants were exposed to all 528 images and were asked to label the image with the emotions that best represent it. Their results show that five out of six emotions were quite weel recognized independently from the angle of view. However, disgust was in some postures confused with fear.

The main drawback of all these projects is the use of video recorded sequences, which misses the impact related to a complete physical experience. 

\subsection{Robotic Studies}
%%%%%%%%%%%%%%%%%%%%%%

The direct consequence of the abundance of works in face elicitation in humans is the amount of works done in Human-Robot Interaction (HRI) focused also on faces. One of the most well-known expressive robots is Kismet~\cite{Breazeal2002}, a robotic face able to interact with people and to show emotions. The face had enough degrees of freedom to portray the basic emotions suggested by Ekman~\cite{Ekman2004} (\textit{happiness}, \textit{surprise}, \textit{anger}, \textit{disgust}, \textit{fear}, and \textit{sadness}) plus \textit{interest}. The face's physical design was done so to invite humans to treat it as if it was a social creature. The interaction studies done with this platform were conceived to consider a person as a caregiver and the robot as the receiver; it seems that the system was capable to engage people in a long term interaction. To achieve this goal, the system had six sub-systems: vision, hearing, motivational, behavioral, speaking, and emotion selection system. Despite the complex system behind Kismet, the emotion's projection evaluation was done using videos with a very limited number of participants. In this experiment, each participant was asked to look at seven videos, one for each implemented emotion. After a video was projected, each participant selected the emotion from a list containing  the seven implemented emotions, only. The results showed a percentage of recognition over 57\%.

%%%%%%%%%%%%%%%%%%%%%%%%
Saerbeck and Christoph~\cite{Saerbeck2010} analyzed the relationship between robots' motion characteristics and perceived affect. They first did a literature review to select characteristics that could be used to show affection. From their literature review, they decided to use acceleration and curvature on robot's trajectory. 
Using these two characteristics, they did an experiment with eighteen participants. For the experiment they used two different platforms (iCat and Roomba) to verify if the embodiment had any impact on affection determination. For the Roomba researchers decided to use a circular trajectory in the room. While for the iCat two objects were place in front of it. It starts in a central position to then move to the left  right and back to center. 
To reduce variables' possibilities, they picked three definite values for each variable. Each participant was exposed to both embodiments and all possible variable combinations. To assess participants' perception, they used PANAS~\cite{WatsonClarkTellegen88} and Self-Assessment Manikin (SAM)~\cite{Lang2008}. Their results show that there was no significant difference between the two embodiments used and that participants were able to assign different emotions to the movement patterns shown to them. 
More importantly, they found out that acceleration is correlated with the perceived arousal but not with valence. 

%%%%%%%%%%%%%%%%
As part of their work on detecting emotions in humans, with a case of use in robot games, Lourens and Barakova~\cite{BarakovaL10} implemented a set of behaviors to determine the emotion  perceived from movement. The behavior parameters were selected based on the work done by Camurri et al.~\cite{pop00002}. To focus participants' attention on movement, they used the e-puck platform~\cite{Mondada09thee-puck}. 
Their experiment is not fully described, but authors highlighted that the subjects were not given any list with possible emotions, rather they asked participants how they think that the robot was feeling. The results showed a very high recognition rate for the implementations of sadness, nervousness, and fear. The implementations for anger and happiness were confused. 

%%%%%%%%%%%%%%%%
Barakova and collaborators used a closet robot~\cite{Barakova2013}, which does not show any anthropomorphic resemblance, to study new possibilities in social interaction. This closet robot can perceive human presence and react to show behaviours that could be perceived as emotions or mental states. The closet robot had several sensors to detect human state and several lights to convey its behaviours. The robot's behaviours were defined using the Interpersonal Behaviour Circle (ICB)~\cite{Leary57}, which is based on two dimensions (dominance-submission and hate-love). A pilot to test the five implemented behaviours was made. The five behaviours correspond to two implementations of dominance, two of submission, and a neutral behaviour. These behaviours differ from each other in the way that the light is turned on and its intensity. From pilot's results, they selected two most convincing behaviours, one dominant and one submissive. Using these two behaviours, they did an experiment with three different scenarios, in which all the participants were exposed. To measure participants' appreciations, they used the Social Dominance Orientation (SDO)~\cite{ pratto1994social} questionnaire and SAM. Their results suggest that people prefer systems that display submissive behaviors. More importantly, their findings suggest that electronic systems can elicit a type of reactions different from the one expected on theories of interpersonal communication.

%%%%%%%%%%%%%%%%
Using a humanoid platform NAO~\cite{NAO2013}, Ca\~namero and collaborators~\cite{Canamero2010,Beck2010} studied the perception of key body poses designed to show emotions. They suggested that the techniques used to convey emotions in virtual characters could not be used in robots, due to the fact that virtual characters have no physical constraints, while robots are constrained by their physical capabilities~\cite{Saerbeck2007, Canamero2010}. They proposed a set of poses that could be used to express emotions with NAO, paying a particular attention to the contribution of the head. They did two experiments in which they show the participants different poses and each participant had to pick one emotion from the list given to them. The list presented to the subjects included six emotions: pride, happiness, excitement, fear, sadness and anger. Their results showed a recognition rate of 88\%, 85\%, 92\%, 88\%, 73\%, and 73\% for anger, sadness, fear, pride, happiness and excitement respectively. The major finding from their experiments is that moving the head up improved the identification of pride, happiness, and excitement. While moving the head down improved the identification of anger and sadness. However, these key poses are taken statically, without any displacement of the robot in the environment.

%%%%%%%%%%%%%%%%%%%%%%
Li and Chignell~\cite{Li2011} used a teddy bear robot capable to move its arms and head to study their contribution on emotion projection. To achieve this, they did a total of four studies. In the first one, four participants were asked to create a gesture that the robot would do in one of the twelve scenarios presented to them. These gestures were recorded and used in the second study. The experiment consisted of two parts: one just showing the gestures, and the second one describing a scenario were the gestures were generated. The participants had to select from a list the emotion that they thought the robot was conveying. The list included basic emotions and mental states, which were selected from the comments obtained during the first study. The results showed that giving a context increased the recognition rate. In the third experiment, they asked five novices and five puppeteers to create gestures for each of the six Ekman's basic emotions. The gestures generated were recorded and used as input in their last test, where the subjects had to select an emotion from a list including the names of the six basic Ekman's emotions. Their results showed that it is possible to convey emotions just using movements in the head and arms. Also they found out that the gestures made by the puppeteers had a better recognition for disgust and fear. Although they presented numerical information about their findings, this information is shown in a way that is not possible to discern the recognition rate of each emotion for each experiment. 

%%%%%%%%%%%%%%%%%%%%%%
Daryl~\cite{Arras2012} is an anthropomorphic robot, without facial expression, nor limbs, but with mobility capabilities. This platform was used to test whether it is possible to project emotions without using cues used in a human-like platform (e.g., tail, ears and head). This robot has head, ears, ability to generate colors in a RGB-LED set positioned in its chest, and a speaker system. The head had no capabilities to show facial expressions, but its movements and robot translations were used to show emotions. The emotions implemented were: happiness, sadness, fear, curiosity, disappointment, and embarrassment. During the experiments done by the researchers, special attention was put to the final distance between the robot and the subject, but the approaching velocity and the followed trajectory were not taken into consideration. The subjects were exposed to a sequence of movements. For each sequence the subject was asked to rate the intensity of each emotion enlisted in a five-point Likert scale questionnaire. The enlisted emotions were the six implemented plus anger, disgust and surprise. Their results showed that participants gave a higher intensity to the desired emotion, also similar ones, such as sadness and disappointment. 

%%%%%%%%%%%%%%%%%%%%%%%
Using the platform WABIAN-2R, Destephe and collaborators~\cite{Destephe2013b} studied the attribution of emotion to a robot's gait. To obtain the robot's movements, the researchers asked two professional actors to walk in a room conveying anger, happiness, sadness, and fear with different intensities (low, regular, normal, high, and exaggerated). All the actors' walking were recorded using a Cortex motion capture system. These data were later reported to robot's embodiment. To verify their data, the researchers first did a pilot study with just two emotions (happiness and sadness) and thirteen subjects~\cite{Destephe2013}. Each subject was exposed to a series of videos. After each video, the subject was asked to select one emotion from a list (happiness, neutral, or sadness) and to state the intensity of the selected emotion. The videos showed were not made with the real robot, but with a virtual model of the robot. From the results obtained, they decided not to use the low intensity because it had a very low recognition rate. During the experiment they adopted the same procedure, emotion and its intensity, but this time showing all the emotions and intensities, excluding low, to the participants. Their results showed that sadness was recognized 73.81\% (average) of the times with an intensity of 21.43\%, happiness was recognized 66.67\% (average) of the times with an intensity of 30.95\%, anger was recognized 61.9\% of the times with an intensity of 26.19\%, and fear was recognized 83.33\% (average) of the times with an intensity of 28.58\%. These results show that people could reasonably well recognize emotions from the robot's gait. 

%%%%%%%%%%%%%%%%%%%%%%%
Lakatos and Collaborators~\cite{Lakatos2014} take inspiration from human-animal interaction
to create behaviours that could enrich human-robot interaction. They conducted an experiment to analyse the recognition rate of emotion expression, using as inspiration the movements done by dogs to convey emotions. The experiment was done using a Wizard-of-Oz scene and its goal was to determine if people could distinguish two emotions (happiness and fear). They use a game approach to determine participant's appraisal of the robot emotion. This was done in order to avoid asking the participants the emotion they believe the robot is eliciting. Therefore, in the experiment, each participant had two balls (yellow and black) and they could play with the robot using one of this balls at a time. However, one of the balls triggers robot's happiness and the other fear. The dog's favourite ball was changed from participant to participant to avoid any bias for the balls' color. The results of this experiment showed that the participants decided to use more dog's favourite ball to play with the dog, which is consistent with the idea that the participants could discern which ball produced ''happiness'' in the robot.

%%%%%%%%%%%%%%%%% %
Brown and Howard focused their interest on head and arm movements using the DARwIn-OP platform~\cite{Brown2014}. Their hypothesis was that using some principles, that they evaluated as important, it is possible to express happiness and sadness in a way that people could recognize. These principles establish that to show happiness is necessary to quiclkly move robot's head up and arms up. While to express sadness the head goes down, arms down, both slowly. To test their hypothesis they conducted an experiment with thirteen participants. Each participant was exposed to fifteen sequences of poses and he/she had to answer for each sequence a five-point Likert scale (very-sad to very happy, with neutral in the middle). They obtained a 95\%, 59\%, and 94\% of accuracy for happiness, neutral and sadness respectively. Although their results show that their key features are determinant to convey happiness and sadness.
 
%%%%%%%%%%%%%%%%%%%
Sharma and collaborators~\cite{Sharma2013} used a quadrotor to study how different Laban's effort~\cite{Laban1968} parameters could impact on the perception of affection. A professional Laban certified actor was asked to generate 16 different paths, for each one changing one of the four Laban's parameters (space, weight, time, and flow). Each generated path was recorded using the Vicon motion-tracking system. Then, they did an experiment were people were asked to assess each path using the Self-assessment Manikin (SAM)~\cite{Lang2008}. In order to analyse the results, they mapped them on the circumplex model of emotion, and evaluated the contribution of each Laban's parameter in the 2-D circumplex model (arousal and valence). The results show that it is possible to increase both valence and arousal by using a more indirect space, or by performing the motion more quickly, and to decrease valence or arousal by a more direct use of space, or a more sustained motion. 
Although they suggest the use of Laban's model as a tool to specify affection movements in Human-Robot Interaction, how to use these parameters in the actual implementation remains an open question, since Laban defined them qualitatively, with reference to human people, and they leave very open questions about the most appropriate numerical values to be used with robots.

%%%%%%%%%%%%%%%%%%%
Suk and collaborators studied the human emotional interpretation for speed, smoothness, granularity and volume of a non-human or animal like object~\cite{NAM2014}. Two experiments were designed to determine the relationship among these features and the emotional interpretation. To assess participants' emotional response they used SAM. The first experiment was focused on speed and smoothness movement features, selecting five different values for each one of these two features. Each participant was exposed to twenty five movements. After a participant observed a movement, the participant marked the two SAM graphic figures (pleasure and arousal).
The results from this first experiment show that the arousal increases as speed increases, but that there is not any clear tendency for smoothness. In the second experiment they evaluated the other two features (granularity and volume) using the same procedure followed in the first experiment. Their results show that granularity is positively correlated with pleasure and arousal, while volume is negatively correlated with pleasure and positively correlated with arousal. As overall result, they found a major contribution of speed on arousal and minor contribution of granularity and volume.

%%%%%%%%%%%%%%%%%%%
To improve the coordination between humans and robots Novikova and Watss~\cite{Novika2015} proposed the use of emotion. They did studies using a own built platform called E4 to verify whether people could detect emotions from a non-human like platform. The E4 platform was constructed using a Lego Mindstorms NTX and it was based on a Phobot robot's design. In all their experiments they used six emotional expressions (scared, surprised, excited, angry, happy, sad) and neutral. The emotion list used in their experiments was balanced using the 2-D circumplex model (arousal and valence), selecting three from each quadrant for a total of twelve emotions, plus the options of ``other'' and ``don't know''. They obtained a recognition rate of 52\%, 42\%, 41\%, 36\% and 15\% for surprise, fear, sadness, happiness, and anger respectively.

These works presented in this section provide guidance about features that could be exploited to project certain emotions. However, range of values to convey specific emotions will guide other researchers in what values could be used to convey their desire emotion or what values not to use because they could be misinterpreted as negative emotions.  

%TODO Maybe the table could summarize what presented. It would also be nice if you could say some words either at the end of each presentation or at the begining, so to justify why you have selected them and not others, and what they bring as important for your work.

%%%%%%%%%%%%%%%%%%
%Table~\ref{table:comparison_work_emotion_projection} summarizes all the robotics studies to convey emotion that we considered, highlighting the following characteristics:
%\begin{itemize}
%	\item \textit{Embodiment} tells if the robot is human-like, robot-like or neither of these two;
%	\item \textit{Platform used} gives the name of the platform used in the study;
%	\item The\textit{Locomotion} criterion was added to put in evidence whether the platform uses displacement movement to express emotion;
%	\item \textit{Emotion Evaluation Method} refers to the methodology used to collect the data in the studies.
%	\item \textit{Emotion presentation} tells if a real robot was used in the study, or any other method;
%	\item \textit{Emotions Implemented} gives the list of emotions showed to the subjects;
%	\item \textit{Type of test} tells if it was done an experiment or a case study.
%	\item \textit{How is the emotion conveyed?} This tells what medium was used to convey the emotion. The media considered were: movement, body posture and face poses.
%\end{itemize}
%\begin{table}[h]
%\caption{Comparison among works on emotion projection in robotics. NA = Not Available}
%\label{table:comparison_work_emotion_projection}
%\begin{center}
%\tiny %\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
%\begin{tabular}{|p{1.0 cm}|p{0.5 cm}|p{0.9 cm}|p{0.8 cm}|p{0.8 cm}|p{1.1 cm}|p{0.9 cm}|p{1.1 cm}|p{0.8 cm}|p{0.8 cm}|}
%%\begin{tabular}{|p{1.5 cm}|p{1.0 cm}|p{1.5 cm}|p{1.5 cm}|p{1.5 cm}|p{2.0 cm}|p{1.5}|p{2.0 cm}|p{1.0 cm}|p{1.5 cm}|}
%\hline 
%\textbf{Work} & \textbf{Year} & \textbf{Embodiment} & \textbf{Platform Used} & \textbf{Locomotion} & \textbf{Emotion Evaluation Method} & \textbf{Emotion Presentation} & \textbf{Emotions Implemented} &  \textbf{Type of test} & \textbf{How is the emotion conveyed?}\\ 
%\hline 
%Breazeal~\cite{Breazeal2002} & 2002 & Face & Kismet & NA & Questionnaire & Video & Happiness, surprise, anger, disgust, fear, sadness and interest & Experiment & Face poses\\ 
%\hline
%Saerbeck and Christoph~\cite{Saerbeck2010} &2010& Animal/ Non-Human like& iCat / Roomba & NA / Differential &PANAS and SAM & Real Robot & NA & Experiment & Movement\\
%\hline 
%Ca\~namero and collaborators~\cite{Canamero2010,Beck2010}& 2010 &Human-Like & NAO & Bipedal & Questionnaire & Real robot & Anger, sadness, fear, pride, happiness, and excitement & Experiment & Body poses\\
%\hline
%Li and Chignell~\cite{Li2011}&2011&Animal-Like& Teddy Bear Robot& NA & Questionnaire & Video (Real robot) & Random + Anger, disgust, fear, happiness, sadness, and surprise & Experiment & Body poses\\ 
%\hline
%Barakova and collaborators~\cite{Barakova2013} & 2013 & NA & Closet robot& NA & SDO and SAM & Real robot & NA & Experiment & Changing lights on and intensity\\ 
%\hline
%Sharma and collaborators~\cite{Sharma2013}& 2013 & Non-Human/Animal & Quadrotor & Aerial & SAM + Questionnaire + Interview &  Real Robot & Laban's poles & Experiment & Movement\\
%\hline
%Destephe and collaborators~\cite{Destephe2013b}&2013&Human-like & WABIAN-2R & Bipedal & Questionnaire & Video (Virtual Robot) & Fear, anger, happiness, and sadness & Experiment & Movement (Gait)+Body Poses\\
%\hline 
%Embgen and collaborators~\cite{Arras2012} & 2014 & Human-like & Daryl &  Differential& Questionnaire & Real Robot & Happiness, sadness, fear, curiosity, embarrassment, and disappointment & Experiment & Movement + Body poses \\ 
%\hline
%Lakatos and Collaborators~\cite{Lakatos2014}& 2014 & Animal-Like & Dog & Holonomic & Indirect (Interaction with the robot) & Real Robot & Happiness and Fear & Experiment & Move + Body poses \\
%\hline
%Brown and Howard~\cite{Brown2014} & 2014& Human-Like & DARwIn-OP & Bipedal & Questionnaire & Real Robot & Happiness and sadness & Experiment & Body poses\\
%\hline
%Suk and collaborators ~\cite{NAM2014}& 2014 &  Non-Human/Animal & Self-made & NA & SAM & Real Robot & NA & Experiment & Speed, smoothness, granularity and volume\\
%\hline
%Novikova and Watss~\cite{Novika2015}& 2015&Non-Human/Animal & Lego & Differential& Questionnaire& Real Robot & Scared, surprise, excited, angry, neutral, happiness and sadness & Experiment& Movement + Poses\\
%\hline 
%\end{tabular} 
%\end{center}
%\end{table}