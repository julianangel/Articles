
Although it is not possible to apply a direct mapping from human studies to robots~\cite{Saerbeck2007,Canamero2010}, human studies provide guidelines about possible features and values that could be used to generate emotional motion. 

\subsection{Human Studies}

Emotion plays an important role in human-human interaction, and can be expressed through diverse channels such as body gestures and poses, body movements, face expressions. Given the complex structure of human face, where more than 43 muscles act, face configurations are considered as a primary channel to express emotion. As a consequence, many works have focused on facial expression, mainly, but not only, influenced by the work done by Ekman~\cite{Ekman2004}. However, the role that body plays in emotion projection was recognized, too, and started to be studied~\cite{Gelder2008,Wallboot1998}. Nevertheless, the amount of works related to body expression of emotions is still small compared to studies about facial expression. 

Analysing some of the few works that have studied human body expressiveness, it is possible to recognize two different methodologies to create the data base of movements to be assessed during the experiments. Applying a first methodology, human actors (either professionals, or amateurs) are asked to walk straight from point A to point B conveying specific emotions~\cite{Dael2012,Meijer1989,Wallboot1998}. Each trial is recorded and later shown to each subjects that have to classify all the sequences. The second methodology uses virtual agents~\cite{Roether2009,Venture2014} to generate the very same set up for the experiments; the agent's movements are generated from the data recorded from human actors.

The work done by Wallboot~\cite{Wallboot1998} has been used as a reference by others. It addressed the question: \textit{Are specific body postures indicative of emotion or are they only used to indicate the intensity of the emotion?} To answer this question, he recorded 224 videos for joy, happiness, sadness, despair, fear, terror, coldness anger, hot anger, disgust, contempt, shame, guilt, pride, and boredom, and showed them to the subjects. His results reaffirm that movement and body postures are indicative of intensity for some emotions. But at the same time, these two characteristics seem to be enough to identify other emotions. 

Complementary studies using virtual agents have been performed by  Kluwer and collaborators~\cite{Kluwer2004}, who studied the contribution of postures and angle view in the interpretation of emotions. They generated 176 static positions for happiness, anger, disgust, fear, sadness and surprise. Each image was later rendered from three different angles (front, left, and above and behind left shoulder) producing a total of 528 images. All the participants were exposed to all 528 images and were asked to label the image with the emotions that best represent it. Their results show that five out of six emotions were quite well recognized independently from the angle of view., while disgust was for some postures confused with fear.

The main drawback of all these projects is the use of video recorded sequences, which misses the impact related to a complete physical experience. For instance, it is clearly different facing an angry robot really rushing against us, or looking at a video where this happens.

\subsection{Robotic Studies}
%%%%%%%%%%%%%%%%%%%%%%

As a direct consequence of the abundance of works in face elicitation in humans, most of the works done in Human-Robot Interaction (HRI) have focused also on faces. One of the most well-known expressive robots is Kismet~\cite{Breazeal2002}, a robotic face able to interact with people and to show emotions. The face had enough degrees of freedom to portray the basic emotions suggested by Ekman~\cite{Ekman2004} (happiness, surprise, anger, disgust, fear, and sadness), plus interest. 
Despite the complex system behind Kismet, the emotion's projection evaluation was done using videos with a very limited number of participants. Similar approach was followed by Li and Chignell~\cite{Li2011}, who used videos of a teddy bear robot to study the contribution of arms and head movement to express emotions. In same direction Destephe and collaborators~\cite{Destephe2013b,Destephe2013} studied the attribution of emotion to a robot's gait using a virtual representation of the platform WABIAN-2R. More recently Knight and Simmons~\cite{knight2016} used Keepon and NAO platforms to study the possibility to project inner states with just head movements. Although use of videos has the advantage to cover a major number of participants, the lack of interaction with the real platform make these works lose the impact that a robot could generate on the participants.  

%%%%%%%%%%%%%%%%%%%%%%%%
The use of real platforms to study emotion projection could be dived in two lines: using anthropomorphic and non-anthropomorphic platforms. In the first case, these works are characterized by the use of cue positions to project desire emotions~\cite{NAO2013}. In some cases, special attention has been taken to determine head's angle and arms position contribution in specific emotions~\cite{Brown2014}. 
Nevertheless, current humanoid platforms cannot generate smooth gaits, which limit the study of body movements. To overcome this limitation, some researchers have reduce the human appearance (e.g. eliminating limps and facial expressions)to increase platform mobility and study new mechanisms to project emotions~\cite{Arras2012}. Pushing forward the reduction on anthropomorphic features, Saerbeck and Christoph used a Roomba platform to study the contribution of curvature in a trajectory to conceive emotional states~\cite{Saerbeck2010}. Similarly Lourens and Barakova~\cite{BarakovaL10} implemented a set of behaviors to determine the emotion  perceived from diverse movements, which were selected from the  work done by Camurri et al.~\cite{pop00002}. Continuing with her research on how robotics' behaviours are interpreted by people, Barakova and collaborators~\cite{Barakova2013} created a closet in which lights could be manipulated to convey pre-defined behaviours. The robot's behaviours were defined using the Interpersonal Behaviour Circle (ICB)~\cite{Leary57}, which is based on two dimensions (dominance-submission and hate-love). Their findings suggest that electronic systems can elicit a type of reactions different from the one expected by theories of interpersonal communication.

Other approaches have tried to get a better understanding of the contribution of diverse features to express emotions through movement. For example, Suk and collaborators payed a particular attention to speed, smoothness, granularity of movement path and volume of a non-bioinspired object~\cite{NAM2014}. Their results suggest that arousal increases as speed increases and that there is not any clear tendency for smoothness. On the other hand, granularity is positively correlated with pleasure and arousal, while volume is negatively correlated with pleasure and positively correlated with arousal. Alike, Tan and collaborators~\cite{Tan2016} have studied the contribution of velocity, fluidity, direction and orientation of a small box. Their results suggest that direction is directly correlated with dominance, but that fluidity does not influence the perception. While flat orientation is related to positive valence, leaning position are related with negative valance. Finally the velocity is correlated with valence, arousal and dominance.  

Due to the popularity that quadrocoptors have received in the last years, Sharma and collaborators~\cite{Sharma2013} used a quadrotor to study how different Laban's effort~\cite{Laban1968} parameters could impact on the perception of affection. A professional Laban certified actor was asked to generate 16 different paths, for each one changing one of the four Laban's parameters (space, weight, time, and flow). Each generated path was recorded using the Vicon motion-tracking system. Continuing with the use of quadrocoptors, Cauchard and collaborators~\cite{Cauchard2016} studied how flight paths could project personal traits and emotional attributes. All these works present a very nice starting to point to identify features and values that could be used to project emotions in robotics, which could help in coordinating humans and robots~\cite{Novika2015}. However, all of these works not give a price guideline to elicit precise emotions, which from our previous case studies could lead to the misinterpretation of movements emotions~\cite{Angel2016}. For example people could confuse an implementation of \textit{Happiness} with \textit{Anger}.