The use of emotion enrichment to improve human robot interaction is not new. There have been several researchers that have enhanced their social robots with emotions or studied how to convey emotions in robotic platforms~\cite{Li2011,Brown2014}. One of the first, well-known expressive robots is Kismet~\cite{Breazeal2002}, a robotic face able to interact with people and to show emotions. This platform uses a specific set of movements based on the Ekman's studies on human emotion expression~\cite{Ekman2004}. Other approaches have tried to use anthropomorphic~\cite{Arras2012} and human-like platforms to convey emotions to study the response of people towards the robot. However, the emotions portrayed were hand-coded, hard-wired to the respective platforms, and their parametrization is not available.

On the other hand, studies focused on entertainment robotics have tried to introduce emotional actions to improve the audience's experience. Breazeal and collaborators~\cite{Breazeal2003} used one robot on the stage. Their anemone-like robot had few behaviours, which included getting scared when a person comes too close; the robot was able to show some basic emotions (i.e., fear and interest). Knight~\cite{Knight2011b} used the platform NAO to produce a sort of stand-up comedy. The robot performs basic actions to add some expressiveness to the joke, but it is not intended to project any emotion. Trying to add some theatrical realism, Breazeal and collaborators~\cite{Breazeal2008} designed and implemented a system to control a lamp. The main characteristic of this lamp is that it could be controlled by just one person, by selecting pre-coded emotions.

Other works in performance robotics have developed systems that do not convey any kind of emotion as \textit{Roboscopie}~\cite{Roboscopie2012}, Fan and collaborators~\cite{Fan2009}, and adaptation of Shakespeare's  Midsummer Night's Dream~\cite{murphy2011} use robots in performances with real actors, but without any emotional expression. Although these works have expressed emotions in their studies, they did not focus on creating a framework that could be used by other researchers in HRI. Therefore, the use of affective architectures for virtual agents (e.g. SAIBA/BML framework~\cite{Kopp2006}) could be though as a feasible solution. However, these frameworks describe high level actions without giving relevant details to its implementation in physical platforms. This gap cannot be filled by making a direct mapping between the abstract actions and the physical platforms. This is due to the fact that virtual agents do not have physical constraints, while robots are constrained by their physical capabilities~\cite{Saerbeck2007,Canamero2010}. 
